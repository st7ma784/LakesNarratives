from test_tube import SlurmCluster
from test_tube import HyperOptArgumentParser
#from trainclip_v2 import train as train_clip
import os,sys
from HOparser import parser
    #overwrte the run function to remove srun



class myCluster(SlurmCluster):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
    def __build_slurm_command(self, trial, slurm_cmd_script_path, timestamp, exp_i, on_gpu):
        #
        #The Goal is to build .... 
        #
        '''

        #!/bin/bash

            # set a job name
            #SBATCH --job-name=second_wandb_trial_batchv0
            #SBATCH --time=24:00:00
            #SBATCH --gres=gpu:1   # also requests portion of CPU and Memory
            #SBATCH --nodes=1
            #SBATCH --signal=USR1@60 
            #SBATCH --mail-type=END,FAIL
            #SBATCH --mail-user=st7ma784@gmail.com
            #SBATCH --account=bdlan05
            #SBATCH --partition=gpu
            export SLURM_NNODES=$SLURM_JOB_NUM_NODES
            export CONDADIR=/nobackup/projects/bdlan05/$USER
            export wandb='9cf7e97e2460c18a89429deed624ec1cbfb537bc'
            source $CONDADIR/miniconda/etc/profile.d/conda.sh
            conda activate $CONDADIR/miniconda/envs/open-ce # ...and activate the conda environment
            python3 /nobackup/projects/bdlan05/smander3/6DimCOCO/trainclip_v37_einsumimp.py  --dir /nobackup/projects/bdlan05/smander3/data
        '''
        job_with_version = '{}v{}'.format(self.job_display_name, exp_i)

        sub_commands =['#!/bin/bash',
            '# Auto-generated by test-tube (https://github.com/williamFalcon/test-tube)',   
            '#SBATCH --time={}'.format(self.job_time),
            '#SBATCH --job-name={}'.format(job_with_version),
            '#SBATCH --nodes={}'.format(self.per_experiment_nb_nodes),
            f'#SBATCH --signal=USR1@{self.minutes_to_checkpoint_before_walltime * 60}',

        ]
        # add job name
        # add out output
        if self.enable_log_out:
            sub_commands.append( '#SBATCH --output={}'.format( os.path.join(self.out_log_path, '{}_slurm_output_%j.out'.format(timestamp))))
        # add err output
        if self.enable_log_err:
            sub_commands.append('#SBATCH --error={}'.format(os.path.join(self.err_log_path, '{}_slurm_output_%j.err'.format(timestamp))))

        # add nb of gpus
        if self.per_experiment_nb_gpus > 0 and on_gpu:
            command = '#SBATCH --gres=gpu:{}'.format(self.per_experiment_nb_gpus)
            if self.gpu_type is not None:
                command ='#SBATCH --gres=gpu:{}:{}'.format(self.gpu_type, self.per_experiment_nb_gpus),    
            sub_commands.append(command)

        # Subscribe to email if requested
        mail_type = []
        if self.notify_on_end:
            mail_type.append('END')
        if self.notify_on_fail:
            mail_type.append('FAIL')
        if len(mail_type) > 0:
            sub_commands.append('#SBATCH --mail-type={}'.format(','.join(mail_type)))
            sub_commands.append('#SBATCH --mail-user={}'.format(self.email))

        sub_commands.extend([ '#SBATCH --{}={}\n'.format(cmd, value) for  (cmd, value, comment) in self.slurm_commands])
        sub_commands.extend(['module load {}'.format(module) for module in self.modules])
        # remove spaces before the hash
        sub_commands = [x.lstrip() for x in sub_commands]

        # add additional commands
        sub_commands.extend(self.commands)
        # add run command
        trial_args = self.__get_hopt_params(trial)
        trial_args = '{} --{} {} --{} {}'.format(trial_args,
                                                    HyperOptArgumentParser.SLURM_CMD_PATH,
                                                    slurm_cmd_script_path,
                                                    HyperOptArgumentParser.SLURM_EXP_CMD,
                                                    exp_i)
        #self.script_name= '/'.join(os.path.realpath(sys.argv[0]).split('/')[:-1]+['trainclip_v37_einsumimp.py'])

        sub_commands.append('{} {} {}'.format(self.python_cmd, self.script_name, trial_args))
        # build full command with empty lines in between
        full_command = '\n'.join(sub_commands)
        print("RUNNING ")
        return full_command





if __name__ == '__main__':
    from trainclip_v37_einsumimp import wandbtrain as train

    argsparser = parser(strategy='random_search')
    hyperparams = argsparser.parse_args()
    # Enable cluster training.
    cluster = myCluster(
        hyperparam_optimizer=hyperparams,
        log_path="/nobackup/projects/bdlan05/smander3/logs/",#hyperparams.log_path,
        python_cmd='python3',
        #test_tube_exp_name="BASELINE CLIP",
    )
    #
    # Email results if your hpc supports it.
    cluster.notify_job_status(
        email='st7ma784@gmail.com', on_done=True, on_fail=True)

    # Add commands to the non-SLURM portion.
    cluster.add_command('export SLURM_NNODES=$SLURM_JOB_NUM_NODES')

    cluster.add_command('export CONDADIR=/nobackup/projects/bdlan05/$USER') # We'll assume that on the BEDE/HEC cluster you've named you conda env after the standard...
    cluster.add_command('export wandb=9cf7e97e2460c18a89429deed624ec1cbfb537bc') # 
    cluster.add_command('source $CONDADIR/miniconda3/etc/profile.d/conda.sh') # ...conda setup script
    cluster.add_command('conda activate $CONDADIR/miniconda/envs/open-ce') # ...and activate the conda environment
    #cluster.add_slurm_cmd(cmd='reservation', value='mcd6_38',comment='reservation for the cluster only ue with blessing of mark Dixon')
    cluster.add_slurm_cmd(
        cmd='account', value='bdlan05', comment='Project account for Bede')
    cluster.add_slurm_cmd(
        cmd='partition', value='gpu', comment='request gpu partition on Bede')
    cluster.per_experiment_nb_cpus=0
    cluster.per_experiment_nb_gpus = 1
    cluster.per_experiment_nb_nodes = 1
    #cluster.gpu_type = '1080ti'

    # set a walltime of 24 hours,0, minues
    cluster.job_time = '24:00:00'

    cluster.minutes_to_checkpoint_before_walltime = 1
    path= os.path.realpath(sys.argv[0]).split('/')[:-1]+['trainclip_v37_einsumimp.py']

    cluster.script_name='/'.join(path)
    # run the models on the cluster
    cluster.optimize_parallel_cluster_gpu(train, nb_trials=1, job_name='fourth_wandb_trial_batch') # Change this to optimize_parralel_cluster_cpu to debug.
